{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import datetime\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import uuid\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import pickle\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import networkx as nx\n",
    "import folium\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phase 3: Smart Solutions Implementation...\n",
      "\n",
      "1. LOADING DATA AND MODELS\n",
      "--------------------------------------------------\n",
      "Data loaded with 367074 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting Phase 3: Smart Solutions Implementation...\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD DATA AND MODELS\n",
    "# =============================================================================\n",
    "print(\"\\n1. LOADING DATA AND MODELS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('data/preprocessed_tollplaza_data.csv')\n",
    "df['initiated_time'] = pd.to_datetime(df['initiated_time'])\n",
    "df['time_interval'] = pd.to_datetime(df['time_interval'])\n",
    "\n",
    "# Load models\n",
    "traffic_model = joblib.load('models/traffic_prediction_model.pkl')\n",
    "anomaly_model, anomaly_scaler = joblib.load('models/traffic_anomaly_models.pkl')\n",
    "\n",
    "# Try to load other models if they exist\n",
    "try:\n",
    "    vc_model = joblib.load('models/vehicle_class_prediction_model.pkl')\n",
    "    vc_model_loaded = True\n",
    "except:\n",
    "    vc_model_loaded = False\n",
    "    print(\"Vehicle class model not found, skipping...\")\n",
    "\n",
    "try:\n",
    "    skip_model, skip_scaler = joblib.load('models/toll_skipping_models.pkl')\n",
    "    skip_model_loaded = True\n",
    "except:\n",
    "    skip_model_loaded = False\n",
    "    print(\"Toll skipping model not found, skipping...\")\n",
    "\n",
    "print(f\"Data loaded with {df.shape[0]} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. LANE OPTIMIZATION SYSTEM\n",
      "--------------------------------------------------\n",
      "Lane recommendations for Devanahalli Toll Plaza at 8:00:\n",
      "{\n",
      "  \"plaza\": \"Devanahalli Toll Plaza\",\n",
      "  \"hour\": 8,\n",
      "  \"expected_traffic\": 3656,\n",
      "  \"lanes_needed\": 37,\n",
      "  \"commercial_ratio\": 0.24699124726477023,\n",
      "  \"recommended_lanes\": [\n",
      "    {\n",
      "      \"lane\": \"15\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 172.7101171458999\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"16\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 146.7950024260068\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"14\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 164.6748046875\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"5\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 178.58856191004998\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"4\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 201.40320765334835\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"13\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 210.99536178107607\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"17\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 331.69315326633165\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"18\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 151.23050095117313\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"8\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 231.17530140110784\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"12\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 190.78646517739816\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 15,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 160.3962641761174\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"7\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 254.56372218476062\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 16,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 130.01327769347498\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 14,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 135.08233009708738\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"2\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 174.781990521327\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"3\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 236.05408805031448\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"9\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 225.44209636517328\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 5,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 225.7092665788318\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 4,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 158.72906178489703\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 17,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 341.1147776183644\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 13,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 175.56069364161849\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 12,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 204.64171897633994\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"6\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 190.84070796460176\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 8,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 337.5933842239186\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 18,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 144.4778578784758\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 7,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 236.2694840834248\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 2,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 254.82395644283122\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"19\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 171.19322459222082\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 9,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 284.42857142857144\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 3,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 337.259649122807\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 6,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 168.2185582822086\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 19,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 183.38957345971565\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"1\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 255.90424242424243\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 1,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 300.085409252669\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"20\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 180.94389438943895\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 20,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 300.19069767441863\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"33\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 145.3109756097561\n",
      "    }\n",
      "  ],\n",
      "  \"vehicle_distribution\": {\n",
      "    \"VC4\": 0.8432713347921226,\n",
      "    \"VC7\": 0.053610503282275714,\n",
      "    \"VC20\": 0.026805251641137857,\n",
      "    \"VC5\": 0.02598468271334792,\n",
      "    \"VC10\": 0.013402625820568928,\n",
      "    \"VC12\": 0.010393873085339168,\n",
      "    \"VC11\": 0.009026258205689279,\n",
      "    \"VC9\": 0.007111597374179431,\n",
      "    \"VC13\": 0.0057439824945295405,\n",
      "    \"VC8\": 0.002461706783369803,\n",
      "    \"VC14\": 0.002188183807439825\n",
      "  },\n",
      "  \"historical_average_processing_time\": 176.9789387308534\n",
      "}\n",
      "\n",
      "Dynamic pricing recommendations for Devanahalli Toll Plaza at 8:00:\n",
      "{\n",
      "  \"plaza\": \"Devanahalli Toll Plaza\",\n",
      "  \"hour\": 8,\n",
      "  \"congestion_level\": 0.7615080191626744,\n",
      "  \"current_pricing\": {\n",
      "    \"VC10\": 355.0,\n",
      "    \"VC11\": 540.0,\n",
      "    \"VC12\": 540.0,\n",
      "    \"VC13\": 540.0,\n",
      "    \"VC14\": 540.0,\n",
      "    \"VC20\": 115.0,\n",
      "    \"VC4\": 115.0,\n",
      "    \"VC5\": 175.0,\n",
      "    \"VC7\": 355.0,\n",
      "    \"VC8\": 540.0,\n",
      "    \"VC9\": 175.0\n",
      "  },\n",
      "  \"recommended_pricing\": {\n",
      "    \"VC10\": 369.56,\n",
      "    \"VC11\": 562.14,\n",
      "    \"VC12\": 562.14,\n",
      "    \"VC13\": 562.14,\n",
      "    \"VC14\": 562.14,\n",
      "    \"VC20\": 119.72,\n",
      "    \"VC4\": 119.72,\n",
      "    \"VC5\": 182.18,\n",
      "    \"VC7\": 369.56,\n",
      "    \"VC8\": 562.14,\n",
      "    \"VC9\": 182.18\n",
      "  },\n",
      "  \"expected_impact\": {\n",
      "    \"revenue_change\": \"4.10%\",\n",
      "    \"expected_traffic_reduction\": \"1.31%\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. LANE OPTIMIZATION SYSTEM\n",
    "# =============================================================================\n",
    "print(\"\\n2. LANE OPTIMIZATION SYSTEM\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "class LaneOptimizationSystem:\n",
    "    def __init__(self, data):\n",
    "        self.df = data\n",
    "        self.plaza_lanes = self._get_plaza_lanes()\n",
    "        self.vehicle_types = sorted(self.df['vehicle_class_code'].unique())\n",
    "        self.lane_efficiency = self._calculate_lane_efficiency()\n",
    "        \n",
    "    def _get_plaza_lanes(self) -> Dict[str, List[str]]:\n",
    "        plaza_lanes = {}\n",
    "        \n",
    "        for plaza in self.df['merchant_name'].unique():\n",
    "            lanes = self.df[self.df['merchant_name'] == plaza]['lane'].unique()\n",
    "            plaza_lanes[plaza] = sorted(lanes, key=lambda x: (isinstance(x, str), x))\n",
    "        return plaza_lanes\n",
    "        \n",
    "    def _calculate_lane_efficiency(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate efficiency of each lane.\"\"\"\n",
    "        lane_stats = self.df.groupby(['merchant_name', 'lane'])['inn_rr_time_sec'].agg([\n",
    "            'mean', 'count', 'std'\n",
    "        ]).reset_index()\n",
    "        lane_stats.columns = ['plaza', 'lane', 'avg_processing_time', 'volume', 'time_std']\n",
    "        \n",
    "        # Normalize and compute a weighted efficiency score (lower is better)\n",
    "        scaler = MinMaxScaler()\n",
    "        lane_stats[['norm_time', 'norm_volume']] = scaler.fit_transform(\n",
    "            lane_stats[['avg_processing_time', 'volume']])\n",
    "        \n",
    "        lane_stats['efficiency_score'] = 0.7 * lane_stats['norm_time'] - 0.3 * lane_stats['norm_volume']\n",
    "        return lane_stats\n",
    "    \n",
    "    def get_lane_recommendations(self, plaza: str, hour: int, expected_traffic: int = None) -> Dict[str, Any]:\n",
    "\n",
    "        # Filter for specific plaza\n",
    "        plaza_data = self.df[self.df['merchant_name'] == plaza].copy()\n",
    "        if plaza_data.empty:\n",
    "            return {\"error\": f\"No data available for plaza {plaza}\"}\n",
    "        \n",
    "        # Get available lanes\n",
    "        available_lanes = self.plaza_lanes.get(plaza, [])\n",
    "        if not available_lanes:\n",
    "            return {\"error\": f\"No lanes data available for plaza {plaza}\"}\n",
    "        \n",
    "        # Get historical traffic patterns for this hour\n",
    "        hourly_data = plaza_data[plaza_data['initiated_time'].dt.hour == hour]\n",
    "        if hourly_data.empty:\n",
    "            return {\"error\": f\"No data available for plaza {plaza} at hour {hour}\"}\n",
    "        \n",
    "        # Convert 'vehicle_comvehicle' to numeric if present\n",
    "        if 'vehicle_comvehicle' in hourly_data.columns:\n",
    "            # Replace 'F' with 0 and 'T' with 1; non-convertible values become NaN\n",
    "            hourly_data['vehicle_comvehicle'] = pd.to_numeric(\n",
    "                hourly_data['vehicle_comvehicle'].replace({'F': 0, 'T': 1}),\n",
    "                errors='coerce'\n",
    "            )\n",
    "            commercial_ratio = hourly_data['vehicle_comvehicle'].mean()\n",
    "            if pd.isna(commercial_ratio):\n",
    "                commercial_ratio = 0.3\n",
    "        else:\n",
    "            commercial_ratio = 0.3\n",
    "        \n",
    "        # Get vehicle distribution for this hour\n",
    "        vehicle_dist = hourly_data['vehicle_class_code'].value_counts(normalize=True)\n",
    "        \n",
    "        # Get lane efficiency for this plaza\n",
    "        plaza_efficiency = self.lane_efficiency[self.lane_efficiency['plaza'] == plaza].copy()\n",
    "        \n",
    "        # Calculate needed lanes based on historical or expected traffic\n",
    "        if expected_traffic is None:\n",
    "            expected_traffic = len(hourly_data)\n",
    "        \n",
    "        lanes_needed = max(2, int(np.ceil(expected_traffic / 100)))\n",
    "        lanes_needed = min(lanes_needed, len(available_lanes))  # Can't open more lanes than available\n",
    "        \n",
    "        # Select the most efficient lanes\n",
    "        best_lanes = plaza_efficiency.sort_values('efficiency_score').head(lanes_needed)\n",
    "        \n",
    "        # Determine which lanes to allocate for commercial vehicles\n",
    "        commercial_lanes = max(1, int(np.round(commercial_ratio * lanes_needed)))\n",
    "        \n",
    "        # Sort lanes by efficiency and assign roles\n",
    "        recommended_lanes = []\n",
    "        for i, (_, lane_data) in enumerate(best_lanes.iterrows()):\n",
    "            lane_role = \"Commercial\" if i < commercial_lanes else \"Non-commercial\"\n",
    "            recommended_lanes.append({\n",
    "                \"lane\": lane_data['lane'],\n",
    "                \"role\": lane_role,\n",
    "                \"expected_volume\": int(expected_traffic / lanes_needed),\n",
    "                \"processing_time\": float(lane_data['avg_processing_time'])\n",
    "            })\n",
    "        \n",
    "        # Prepare recommendations\n",
    "        recommendations = {\n",
    "            \"plaza\": plaza,\n",
    "            \"hour\": hour,\n",
    "            \"expected_traffic\": expected_traffic,\n",
    "            \"lanes_needed\": lanes_needed,\n",
    "            \"commercial_ratio\": float(commercial_ratio),\n",
    "            \"recommended_lanes\": recommended_lanes,\n",
    "            \"vehicle_distribution\": {k: float(v) for k, v in vehicle_dist.items()},\n",
    "            \"historical_average_processing_time\": float(hourly_data['inn_rr_time_sec'].mean())\n",
    "        }\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    \n",
    "    def get_dynamic_pricing_recommendations(self, plaza: str, hour: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get dynamic pricing recommendations for a specific plaza and time.\n",
    "        \"\"\"\n",
    "        plaza_data = self.df[self.df['merchant_name'] == plaza].copy()\n",
    "        if plaza_data.empty:\n",
    "            return {\"error\": f\"No data available for plaza {plaza}\"}\n",
    "            \n",
    "        hourly_data = plaza_data[plaza_data['initiated_time'].dt.hour == hour]\n",
    "        if hourly_data.empty:\n",
    "            return {\"error\": f\"No data available for plaza {plaza} at hour {hour}\"}\n",
    "        \n",
    "        max_hourly_traffic = plaza_data.groupby(plaza_data['initiated_time'].dt.hour).size().max()\n",
    "        current_hourly_traffic = len(hourly_data)\n",
    "        congestion_level = current_hourly_traffic / max_hourly_traffic if max_hourly_traffic > 0 else 0\n",
    "        \n",
    "        current_pricing = {}\n",
    "        for vc in self.vehicle_types:\n",
    "            vc_data = hourly_data[hourly_data['vehicle_class_code'] == vc]\n",
    "            if not vc_data.empty:\n",
    "                current_pricing[vc] = float(vc_data['txn_amount'].median())\n",
    "        \n",
    "        recommended_pricing = {}\n",
    "        for vc, base_price in current_pricing.items():\n",
    "            if congestion_level > 0.7:  # High congestion: increase up to 20%\n",
    "                factor = 1.0 + (congestion_level - 0.7) * (0.2 / 0.3)\n",
    "            elif congestion_level < 0.3:  # Low congestion: decrease up to 10%\n",
    "                factor = 1.0 - (0.3 - congestion_level) * (0.1 / 0.3)\n",
    "            else:\n",
    "                factor = 1.0\n",
    "            recommended_pricing[vc] = round(base_price * factor, 2)\n",
    "        \n",
    "        recommendations = {\n",
    "            \"plaza\": plaza,\n",
    "            \"hour\": hour,\n",
    "            \"congestion_level\": float(congestion_level),\n",
    "            \"current_pricing\": current_pricing,\n",
    "            \"recommended_pricing\": recommended_pricing,\n",
    "            \"expected_impact\": {\n",
    "                \"revenue_change\": f\"{(sum(recommended_pricing.values()) - sum(current_pricing.values())) / sum(current_pricing.values()) * 100:.2f}%\",\n",
    "                \"expected_traffic_reduction\": f\"{5 * (congestion_level - 0.5) if congestion_level > 0.5 else 0:.2f}%\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def visualize_lane_recommendations(self, plaza: str, hour: int) -> Dict[str, Any]:\n",
    "    \n",
    "        recommendations = self.get_lane_recommendations(plaza, hour)\n",
    "        if \"error\" in recommendations:\n",
    "            return {\"error\": recommendations[\"error\"]}\n",
    "        \n",
    "        viz_data = {\n",
    "            \"plaza\": plaza,\n",
    "            \"hour\": hour,\n",
    "            \"lane_allocation\": {\n",
    "                \"lanes\": [lane[\"lane\"] for lane in recommendations[\"recommended_lanes\"]],\n",
    "                \"roles\": [lane[\"role\"] for lane in recommendations[\"recommended_lanes\"]],\n",
    "                \"volumes\": [lane[\"expected_volume\"] for lane in recommendations[\"recommended_lanes\"]]\n",
    "            },\n",
    "            \"vehicle_distribution\": recommendations[\"vehicle_distribution\"]\n",
    "        }\n",
    "        \n",
    "        return viz_data\n",
    "\n",
    "# Initialize lane optimization system\n",
    "lane_optimizer = LaneOptimizationSystem(df)\n",
    "\n",
    "# Example usage for lane optimization\n",
    "plaza_example = df['merchant_name'].value_counts().index[0]  # Most common plaza\n",
    "hour_example = 8  # Example hour\n",
    "\n",
    "lane_recommendations = lane_optimizer.get_lane_recommendations(plaza_example, hour_example)\n",
    "print(f\"Lane recommendations for {plaza_example} at {hour_example}:00:\")\n",
    "print(json.dumps(lane_recommendations, indent=2))\n",
    "\n",
    "pricing_recommendations = lane_optimizer.get_dynamic_pricing_recommendations(plaza_example, hour_example)\n",
    "print(f\"\\nDynamic pricing recommendations for {plaza_example} at {hour_example}:00:\")\n",
    "print(json.dumps(pricing_recommendations, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. AUTOMATED INSIGHTS GENERATOR\n",
      "--------------------------------------------------\n",
      "Generating automated insights...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 3. AUTOMATED INSIGHTS GENERATOR\n",
    "# =============================================================================\n",
    "print(\"\\n3. AUTOMATED INSIGHTS GENERATOR\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "class AutomatedInsightsGenerator:\n",
    "    def __init__(self, data, openai_api_key=None):\n",
    "        self.df = data\n",
    "        self.client = None\n",
    "        if openai_api_key:\n",
    "            try:\n",
    "                self.client = OpenAI(api_key=openai_api_key)\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing OpenAI client: {e}\")\n",
    "                print(\"Automated insights will be generated without OpenAI.\")\n",
    "    \n",
    "    def _get_basic_stats(self) -> Dict[str, Any]:\n",
    "        stats = {\n",
    "            \"total_transactions\": len(self.df),\n",
    "            \"total_plazas\": self.df['merchant_name'].nunique(),\n",
    "            \"total_vehicles\": self.df['vehicle_regn_number'].nunique(),\n",
    "            \"total_revenue\": float(self.df['txn_amount'].sum()),\n",
    "            \"avg_transaction_amount\": float(self.df['txn_amount'].mean()),\n",
    "            \"busiest_plaza\": self.df['merchant_name'].value_counts().index[0],\n",
    "            \"busiest_hour\": self.df.groupby(self.df['initiated_time'].dt.hour).size().idxmax(),\n",
    "            \"most_common_vehicle_type\": self.df['vehicle_class_code'].value_counts().index[0]\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def _get_traffic_insights(self) -> Dict[str, Any]:\n",
    "        hourly_traffic = self.df.groupby(self.df['initiated_time'].dt.hour).size()\n",
    "        peak_hour = hourly_traffic.idxmax()\n",
    "        off_peak_hour = hourly_traffic.idxmin()\n",
    "        morning_traffic = hourly_traffic.loc[6:12].sum()\n",
    "        evening_traffic = hourly_traffic.loc[16:20].sum()\n",
    "        night_traffic = hourly_traffic.loc[[*range(0, 6), *range(21, 24)]].sum()\n",
    "        plaza_traffic = self.df['merchant_name'].value_counts()\n",
    "        busiest_plaza = plaza_traffic.index[0]\n",
    "        quietest_plaza = plaza_traffic.index[-1]\n",
    "        direction_traffic = self.df['direction'].value_counts(normalize=True)\n",
    "        main_direction = direction_traffic.index[0]\n",
    "        main_direction_pct = float(direction_traffic.iloc[0] * 100)\n",
    "        \n",
    "        insights = {\n",
    "            \"peak_hour\": int(peak_hour),\n",
    "            \"peak_hour_traffic\": int(hourly_traffic[peak_hour]),\n",
    "            \"off_peak_hour\": int(off_peak_hour),\n",
    "            \"off_peak_hour_traffic\": int(hourly_traffic[off_peak_hour]),\n",
    "            \"peak_to_offpeak_ratio\": float(hourly_traffic[peak_hour] / hourly_traffic[off_peak_hour]),\n",
    "            \"morning_vs_evening\": {\n",
    "                \"morning_traffic\": int(morning_traffic),\n",
    "                \"evening_traffic\": int(evening_traffic),\n",
    "                \"ratio\": float(morning_traffic / evening_traffic) if evening_traffic > 0 else float('inf')\n",
    "            },\n",
    "            \"night_traffic_percentage\": float(night_traffic / hourly_traffic.sum() * 100),\n",
    "            \"busiest_plaza\": busiest_plaza,\n",
    "            \"busiest_plaza_transactions\": int(plaza_traffic[busiest_plaza]),\n",
    "            \"quietest_plaza\": quietest_plaza,\n",
    "            \"quietest_plaza_transactions\": int(plaza_traffic[quietest_plaza]),\n",
    "            \"main_travel_direction\": main_direction,\n",
    "            \"main_direction_percentage\": main_direction_pct\n",
    "        }\n",
    "        return insights\n",
    "    \n",
    "    def _get_vehicle_insights(self) -> Dict[str, Any]:\n",
    "        vehicle_dist = self.df['vehicle_class_code'].value_counts(normalize=True)\n",
    "        \n",
    "        if 'vehicle_comvehicle' in self.df.columns:\n",
    "            # Convert the column to numeric: 'F' -> 0, 'T' -> 1, non-convertible values become NaN\n",
    "            vc_series = pd.to_numeric(\n",
    "                self.df['vehicle_comvehicle'].replace({'F': 0, 'T': 1}),\n",
    "                errors='coerce'\n",
    "            )\n",
    "            commercial_pct = float(vc_series.mean() * 100)\n",
    "        else:\n",
    "            commercial_classes = ['VC5', 'VC6', 'VC7', 'VC8', 'VC10', 'VC11', 'VC12', 'VC13', 'VC14', 'VC15', 'VC16', 'VC17', 'VC20']\n",
    "            commercial_pct = float(self.df['vehicle_class_code'].isin(commercial_classes).mean() * 100)\n",
    "        \n",
    "        revenue_by_class = self.df.groupby('vehicle_class_code')['txn_amount'].sum()\n",
    "        top_revenue_class = revenue_by_class.idxmax()\n",
    "        avg_by_class = self.df.groupby('vehicle_class_code')['txn_amount'].mean().sort_values(ascending=False)\n",
    "        \n",
    "        insights = {\n",
    "            \"top_vehicle_class\": vehicle_dist.index[0],\n",
    "            \"top_vehicle_class_percentage\": float(vehicle_dist.iloc[0] * 100),\n",
    "            \"commercial_vehicle_percentage\": commercial_pct,\n",
    "            \"top_revenue_vehicle_class\": top_revenue_class,\n",
    "            \"top_revenue_vehicle_class_amount\": float(revenue_by_class[top_revenue_class]),\n",
    "            \"highest_fare_vehicle_class\": avg_by_class.index[0],\n",
    "            \"highest_fare_amount\": float(avg_by_class.iloc[0]),\n",
    "            \"vehicle_class_distribution\": {k: float(v * 100) for k, v in vehicle_dist.items()}\n",
    "        }\n",
    "        return insights\n",
    "\n",
    "    \n",
    "    def _get_operational_insights(self) -> Dict[str, Any]:\n",
    "        avg_processing = float(self.df['inn_rr_time_sec'].mean())\n",
    "        plaza_processing = self.df.groupby('merchant_name')['inn_rr_time_sec'].mean().sort_values()\n",
    "        fastest_plaza = plaza_processing.index[0]\n",
    "        slowest_plaza = plaza_processing.index[-1]\n",
    "        class_processing = self.df.groupby('vehicle_class_code')['inn_rr_time_sec'].mean().sort_values()\n",
    "        fastest_class = class_processing.index[0]\n",
    "        slowest_class = class_processing.index[-1]\n",
    "        hour_processing = self.df.groupby(self.df['initiated_time'].dt.hour)['inn_rr_time_sec'].mean()\n",
    "        fastest_hour = hour_processing.idxmin()\n",
    "        slowest_hour = hour_processing.idxmax()\n",
    "        \n",
    "        insights = {\n",
    "            \"average_processing_time\": avg_processing,\n",
    "            \"fastest_plaza\": fastest_plaza,\n",
    "            \"fastest_plaza_time\": float(plaza_processing[fastest_plaza]),\n",
    "            \"slowest_plaza\": slowest_plaza,\n",
    "            \"slowest_plaza_time\": float(plaza_processing[slowest_plaza]),\n",
    "            \"fastest_vehicle_class\": fastest_class,\n",
    "            \"fastest_vehicle_class_time\": float(class_processing[fastest_class]),\n",
    "            \"slowest_vehicle_class\": slowest_class,\n",
    "            \"slowest_vehicle_class_time\": float(class_processing[slowest_class]),\n",
    "            \"fastest_hour\": int(fastest_hour),\n",
    "            \"fastest_hour_time\": float(hour_processing[fastest_hour]),\n",
    "            \"slowest_hour\": int(slowest_hour),\n",
    "            \"slowest_hour_time\": float(hour_processing[slowest_hour])\n",
    "        }\n",
    "        return insights\n",
    "    \n",
    "    def _get_revenue_insights(self) -> Dict[str, Any]:\n",
    "        hourly_revenue = self.df.groupby(self.df['initiated_time'].dt.hour)['txn_amount'].sum()\n",
    "        peak_revenue_hour = hourly_revenue.idxmax()\n",
    "        plaza_revenue = self.df.groupby('merchant_name')['txn_amount'].sum().sort_values(ascending=False)\n",
    "        top_revenue_plaza = plaza_revenue.index[0]\n",
    "        plaza_avg_revenue = self.df.groupby('merchant_name')['txn_amount'].mean().sort_values(ascending=False)\n",
    "        highest_avg_plaza = plaza_avg_revenue.index[0]\n",
    "        \n",
    "        insights = {\n",
    "            \"total_daily_revenue\": float(self.df['txn_amount'].sum()),\n",
    "            \"average_transaction_amount\": float(self.df['txn_amount'].mean()),\n",
    "            \"median_transaction_amount\": float(self.df['txn_amount'].median()),\n",
    "            \"peak_revenue_hour\": int(peak_revenue_hour),\n",
    "            \"peak_hour_revenue\": float(hourly_revenue[peak_revenue_hour]),\n",
    "            \"top_revenue_plaza\": top_revenue_plaza,\n",
    "            \"top_plaza_revenue\": float(plaza_revenue[top_revenue_plaza]),\n",
    "            \"top_plaza_revenue_percentage\": float(plaza_revenue[top_revenue_plaza] / plaza_revenue.sum() * 100),\n",
    "            \"highest_average_revenue_plaza\": highest_avg_plaza,\n",
    "            \"highest_average_amount\": float(plaza_avg_revenue[highest_avg_plaza])\n",
    "        }\n",
    "        return insights\n",
    "    \n",
    "    def generate_natural_language_insights(self, data_summary: Dict[str, Any]) -> str:\n",
    "        if self.client:\n",
    "            try:\n",
    "                prompt = f\"\"\"\n",
    "                You are a transportation analytics expert. Based on the following toll plaza data insights, \n",
    "                generate a comprehensive analysis that highlights key patterns, anomalies, and actionable \n",
    "                recommendations for toll plaza operators. Format your response as a bullet-point summary\n",
    "                followed by paragraphs of detailed analysis. Focus on actionable insights.\n",
    "                \n",
    "                DATA INSIGHTS:\n",
    "                {data_summary}\n",
    "                \"\"\"\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[{\"role\": \"system\", \"content\": \"You are a toll plaza analytics expert.\"},\n",
    "                              {\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.5\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error using OpenAI: {e}\")\n",
    "                return self._generate_fallback_insights(data_summary)\n",
    "        else:\n",
    "            return self._generate_fallback_insights(data_summary)\n",
    "    \n",
    "    def _generate_fallback_insights(self, data_summary: Dict[str, Any]) -> str:\n",
    "        traffic = data_summary.get(\"traffic_insights\", {})\n",
    "        vehicle = data_summary.get(\"vehicle_insights\", {})\n",
    "        operation = data_summary.get(\"operational_insights\", {})\n",
    "        revenue = data_summary.get(\"revenue_insights\", {})\n",
    "        \n",
    "        insights = f\"\"\"\n",
    "        # Toll Plaza Analytics Insights\n",
    "\n",
    "        ## Key Highlights\n",
    "        * Total transactions: {data_summary.get('basic_stats', {}).get('total_transactions', 'N/A')}\n",
    "        * Total revenue: ₹{data_summary.get('basic_stats', {}).get('total_revenue', 'N/A'):,.2f}\n",
    "        * Peak hour: {traffic.get('peak_hour', 'N/A')}:00 with {traffic.get('peak_hour_traffic', 'N/A')} transactions\n",
    "        * Busiest plaza: {traffic.get('busiest_plaza', 'N/A')} with {traffic.get('busiest_plaza_transactions', 'N/A')} transactions\n",
    "        \n",
    "        ## Traffic Patterns\n",
    "        The peak hour ({traffic.get('peak_hour', 'N/A')}:00) has {traffic.get('peak_to_offpeak_ratio', 'N/A'):.1f}x more traffic than the off-peak hour ({traffic.get('off_peak_hour', 'N/A')}:00). \n",
    "        Morning traffic is {traffic.get('morning_vs_evening', {}).get('ratio', 'N/A'):.2f}x the evening traffic.\n",
    "        \n",
    "        ## Vehicle Distribution\n",
    "        {vehicle.get('commercial_vehicle_percentage', 'N/A'):.1f}% of vehicles are commercial, with {vehicle.get('top_vehicle_class', 'N/A')} being the most common type.\n",
    "        The {vehicle.get('top_revenue_vehicle_class', 'N/A')} class generates the most revenue at ₹{vehicle.get('top_revenue_vehicle_class_amount', 'N/A'):,.2f}.\n",
    "        \n",
    "        ## Operational Efficiency\n",
    "        Average processing time is {operation.get('average_processing_time', 'N/A'):.2f} seconds.\n",
    "        {operation.get('fastest_plaza', 'N/A')} is the most efficient plaza at {operation.get('fastest_plaza_time', 'N/A'):.2f}s.\n",
    "        \n",
    "        ## Revenue Insights\n",
    "        Total daily revenue is ₹{revenue.get('total_daily_revenue', 'N/A'):,.2f} with an average transaction of ₹{revenue.get('average_transaction_amount', 'N/A'):.2f}.\n",
    "        Peak revenue hour is {revenue.get('peak_revenue_hour', 'N/A')}:00, generating ₹{revenue.get('peak_hour_revenue', 'N/A'):,.2f}.\n",
    "        \n",
    "        ## Recommendations\n",
    "        1. Optimize lane allocation during peak hours.\n",
    "        2. Consider dynamic pricing to balance traffic.\n",
    "        3. Improve processing times at less efficient plazas.\n",
    "        4. Target revenue optimization for {vehicle.get('top_revenue_vehicle_class', 'N/A')} vehicles.\n",
    "        \"\"\"\n",
    "        return insights\n",
    "    \n",
    "    def generate_insights_report(self) -> Dict[str, Any]:\n",
    "        basic_stats = self._get_basic_stats()\n",
    "        traffic_insights = self._get_traffic_insights()\n",
    "        vehicle_insights = self._get_vehicle_insights()\n",
    "        operational_insights = self._get_operational_insights()\n",
    "        revenue_insights = self._get_revenue_insights()\n",
    "        \n",
    "        data_summary = {\n",
    "            \"basic_stats\": basic_stats,\n",
    "            \"traffic_insights\": traffic_insights,\n",
    "            \"vehicle_insights\": vehicle_insights,\n",
    "            \"operational_insights\": operational_insights,\n",
    "            \"revenue_insights\": revenue_insights\n",
    "        }\n",
    "        \n",
    "        narrative = self.generate_natural_language_insights(data_summary)\n",
    "        \n",
    "        report = {\n",
    "            \"summary\": data_summary,\n",
    "            \"narrative\": narrative,\n",
    "            \"generated_at\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"data_date\": self.df['initiated_time'].dt.date.min().strftime(\"%Y-%m-%d\")\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def generate_plaza_insights(self, plaza_name: str) -> Dict[str, Any]:\n",
    "        plaza_data = self.df[self.df['merchant_name'] == plaza_name]\n",
    "        if plaza_data.empty:\n",
    "            return {\"error\": f\"No data found for plaza {plaza_name}\"}\n",
    "        \n",
    "        temp_generator = AutomatedInsightsGenerator(plaza_data, None)\n",
    "        return temp_generator.generate_insights_report()\n",
    "\n",
    "# Check for OpenAI key (implement your own key handling)\n",
    "openai_key = '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'\n",
    "if not openai_key:\n",
    "    print(\"No OpenAI API key found. Using fallback insight generation.\")\n",
    "\n",
    "# Initialize insights generator\n",
    "insights_generator = AutomatedInsightsGenerator(df, openai_key)\n",
    "print(\"Generating automated insights...\")\n",
    "insights_report = insights_generator.generate_insights_report()\n",
    "print(\"\\nInsights Narrative (excerpt):\")\n",
    "narrative_excerpt = \"\\n\".join(insights_report[\"narrative\"].strip().split(\"\\n\"))\n",
    "print(narrative_excerpt + \"\\n...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradi\\AppData\\Local\\Temp\\ipykernel_25196\\3675258760.py:63: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('preprocessed_tollplaza_data.csv')\n",
      "2025-03-19 20:15:49,264 - INFO - Debugging DataFrame...\n",
      "2025-03-19 20:15:49,265 - INFO - Columns: ['SlNo.', 'merchant_name', 'direction', 'lane', 'tag_id', 'vehicle_regn_number', 'txn_amount', 'initiated_time', 'inn_rr_time_sec', 'vehicle_class_code', 'vehicle_comvehicle', 'geocode', 'merchant_sub_type', 'city', 'state', 'hour', 'day_of_week', 'txn_amount_scaled', 'vehicle_class_code_enc', 'merchant_name_enc', 'minute', 'time_of_day', 'latitude', 'longitude', 'merchant_name_encoded', 'direction_encoded', 'lane_encoded', 'vehicle_class_code_encoded', 'merchant_sub_type_encoded', 'city_encoded', 'state_encoded', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'day_sin', 'day_cos', 'time_interval', 'traffic_count', 'is_frequent_traveler']\n",
      "2025-03-19 20:15:49,266 - INFO - Data types: SlNo.                                  int64\n",
      "merchant_name                         object\n",
      "direction                             object\n",
      "lane                                  object\n",
      "tag_id                                object\n",
      "vehicle_regn_number                   object\n",
      "txn_amount                             int64\n",
      "initiated_time                datetime64[ns]\n",
      "inn_rr_time_sec                        int64\n",
      "vehicle_class_code                    object\n",
      "vehicle_comvehicle                    object\n",
      "geocode                               object\n",
      "merchant_sub_type                     object\n",
      "city                                  object\n",
      "state                                 object\n",
      "hour                                   int64\n",
      "day_of_week                            int64\n",
      "txn_amount_scaled                    float64\n",
      "vehicle_class_code_enc                 int64\n",
      "merchant_name_enc                      int64\n",
      "minute                                 int64\n",
      "time_of_day                           object\n",
      "latitude                             float64\n",
      "longitude                            float64\n",
      "merchant_name_encoded                  int64\n",
      "direction_encoded                      int64\n",
      "lane_encoded                           int64\n",
      "vehicle_class_code_encoded             int64\n",
      "merchant_sub_type_encoded              int64\n",
      "city_encoded                           int64\n",
      "state_encoded                          int64\n",
      "hour_sin                             float64\n",
      "hour_cos                             float64\n",
      "minute_sin                           float64\n",
      "minute_cos                           float64\n",
      "day_sin                              float64\n",
      "day_cos                              float64\n",
      "time_interval                 datetime64[ns]\n",
      "traffic_count                          int64\n",
      "is_frequent_traveler                    bool\n",
      "dtype: object\n",
      "2025-03-19 20:15:49,477 - INFO - Missing values: SlNo.                            0\n",
      "merchant_name                    0\n",
      "direction                        0\n",
      "lane                             0\n",
      "tag_id                           0\n",
      "vehicle_regn_number              0\n",
      "txn_amount                       0\n",
      "initiated_time                   0\n",
      "inn_rr_time_sec                  0\n",
      "vehicle_class_code               0\n",
      "vehicle_comvehicle               0\n",
      "geocode                          0\n",
      "merchant_sub_type                0\n",
      "city                             0\n",
      "state                            0\n",
      "hour                             0\n",
      "day_of_week                      0\n",
      "txn_amount_scaled                0\n",
      "vehicle_class_code_enc           0\n",
      "merchant_name_enc                0\n",
      "minute                           0\n",
      "time_of_day                   9093\n",
      "latitude                         0\n",
      "longitude                        0\n",
      "merchant_name_encoded            0\n",
      "direction_encoded                0\n",
      "lane_encoded                     0\n",
      "vehicle_class_code_encoded       0\n",
      "merchant_sub_type_encoded        0\n",
      "city_encoded                     0\n",
      "state_encoded                    0\n",
      "hour_sin                         0\n",
      "hour_cos                         0\n",
      "minute_sin                       0\n",
      "minute_cos                       0\n",
      "day_sin                          0\n",
      "day_cos                          0\n",
      "time_interval                    0\n",
      "traffic_count                    0\n",
      "is_frequent_traveler             0\n",
      "dtype: int64\n",
      "2025-03-19 20:15:49,485 - INFO - First few rows:\n",
      "   SlNo.                 merchant_name direction    lane tag_id  \\\n",
      "0      1  Bannerghatta Road (P2) Plaza         S   P2BX2  HXYUH   \n",
      "1      2  Bannerghatta Road (P2) Plaza         S   P2BX2  JGEFQ   \n",
      "2      3  Bannerghatta Road (P2) Plaza         S   P2BX2  VOAPH   \n",
      "3      4      Kadathanamale Toll Plaza         S  LANE09  XBANK   \n",
      "4      5      Kadathanamale Toll Plaza         S  LANE10  HMWDZ   \n",
      "\n",
      "  vehicle_regn_number  txn_amount      initiated_time  inn_rr_time_sec  \\\n",
      "0               YFUNB         225 2024-03-19 21:54:00              994   \n",
      "1               GQHLZ         225 2024-03-19 18:59:00             4528   \n",
      "2               DNTMI         150 2024-03-19 00:14:00              134   \n",
      "3               ZMAOY          35 2024-03-19 18:19:00               80   \n",
      "4               IDSRZ          20 2024-03-19 15:03:00               64   \n",
      "\n",
      "  vehicle_class_code  ... state_encoded  hour_sin      hour_cos minute_sin  \\\n",
      "0                VC4  ...             0 -0.707107  7.071068e-01  -0.587785   \n",
      "1               VC20  ...             0 -1.000000 -1.836970e-16  -0.104528   \n",
      "2                VC5  ...             0  0.000000  1.000000e+00   0.994522   \n",
      "3               VC20  ...             0 -1.000000 -1.836970e-16   0.913545   \n",
      "4                VC4  ...             0 -0.707107 -7.071068e-01   0.309017   \n",
      "\n",
      "  minute_cos   day_sin  day_cos       time_interval  traffic_count  \\\n",
      "0   0.809017  0.781831  0.62349 2024-03-19 21:45:00           3514   \n",
      "1   0.994522  0.781831  0.62349 2024-03-19 18:45:00           5128   \n",
      "2   0.104528  0.781831  0.62349 2024-03-19 00:00:00           2469   \n",
      "3  -0.406737  0.781831  0.62349 2024-03-19 18:15:00           5598   \n",
      "4   0.951057  0.781831  0.62349 2024-03-19 15:00:00           4562   \n",
      "\n",
      "   is_frequent_traveler  \n",
      "0                 False  \n",
      "1                 False  \n",
      "2                 False  \n",
      "3                 False  \n",
      "4                  True  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     64\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33minitiated_time\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33minitiated_time\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     65\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtime_interval\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mtime_interval\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m skipping_detector = \u001b[43mTollSkippingDetection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m skipping_report = skipping_detector.detect_potential_toll_skipping()\n\u001b[32m     69\u001b[39m \u001b[38;5;129m@app\u001b[39m.route(\u001b[33m'\u001b[39m\u001b[33m/api/toll-skipping\u001b[39m\u001b[33m'\u001b[39m, methods=[\u001b[33m'\u001b[39m\u001b[33mGET\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_toll_skipping\u001b[39m():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mTollSkippingDetection.__init__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.df = debug_dataframe(data)  \u001b[38;5;66;03m# ✅ Ensure self.df is initialized\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mself\u001b[39m.graph = \u001b[38;5;28mself\u001b[39m._build_route_graph()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28mself\u001b[39m.expected_routes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_identify_common_routes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mself\u001b[39m.tag_routes = \u001b[38;5;28mself\u001b[39m._extract_vehicle_routes()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mTollSkippingDetection._identify_common_routes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m edge \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.graph.edges(data=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     36\u001b[39m     source, target, data = edge\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     common_routes[(source, target)] = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall_simple_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ✅ Convert generator to list\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m common_routes\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from flask import Flask, jsonify\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def debug_dataframe(df):\n",
    "    logger.info(\"Debugging DataFrame...\")\n",
    "    logger.info(f\"Columns: {df.columns.tolist()}\")\n",
    "    logger.info(f\"Data types: {df.dtypes}\")\n",
    "    logger.info(f\"Missing values: {df.isna().sum()}\")\n",
    "    logger.info(f\"First few rows:\\n{df.head()}\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "class TollSkippingDetection:\n",
    "    def __init__(self, data):\n",
    "        self.df = debug_dataframe(data)  # ✅ Ensure self.df is initialized\n",
    "        self.graph = self._build_route_graph()\n",
    "        self.expected_routes = self._identify_common_routes()\n",
    "        self.tag_routes = self._extract_vehicle_routes()\n",
    "\n",
    "    def _build_route_graph(self):\n",
    "        G = nx.DiGraph()\n",
    "        for _, group in self.df.groupby('tag_id'):\n",
    "            plazas = group.sort_values('initiated_time')['merchant_name'].tolist()\n",
    "            for i in range(len(plazas) - 1):\n",
    "                G.add_edge(plazas[i], plazas[i + 1], weight=G.get_edge_data(plazas[i], plazas[i + 1], {'weight': 0})['weight'] + 1)\n",
    "        return G\n",
    "\n",
    "    def _identify_common_routes(self):\n",
    "        common_routes = {}\n",
    "        for edge in self.graph.edges(data=True):\n",
    "            source, target, data = edge\n",
    "            common_routes[(source, target)] = list(nx.all_simple_paths(self.graph, source, target))  # ✅ Convert generator to list\n",
    "        return common_routes\n",
    "\n",
    "    def _extract_vehicle_routes(self):\n",
    "        vehicle_routes = {}\n",
    "        for tag_id, group in self.df.groupby('tag_id'):\n",
    "            sorted_group = group.sort_values('initiated_time')\n",
    "            vehicle_routes[tag_id] = list(zip(sorted_group['merchant_name'], sorted_group['initiated_time']))\n",
    "        return vehicle_routes\n",
    "\n",
    "    def detect_potential_toll_skipping(self):\n",
    "        incidents = []\n",
    "        for tag_id, route in self.tag_routes.items():\n",
    "            for i in range(len(route) - 1):\n",
    "                source, target = route[i][0], route[i + 1][0]\n",
    "                if (source, target) in self.expected_routes and len(self.expected_routes[(source, target)]) > 2:\n",
    "                    incidents.append({\n",
    "                        \"tag_id\": tag_id,\n",
    "                        \"source_plaza\": source,\n",
    "                        \"target_plaza\": target,\n",
    "                        \"skipped_tolls\": len(self.expected_routes[(source, target)]) - 2,\n",
    "                        \"estimated_loss\": (len(self.expected_routes[(source, target)]) - 2) * 50\n",
    "                    })\n",
    "        return incidents\n",
    "\n",
    "app = Flask(__name__)\n",
    "df = pd.read_csv('preprocessed_tollplaza_data.csv')\n",
    "df['initiated_time'] = pd.to_datetime(df['initiated_time'])\n",
    "df['time_interval'] = pd.to_datetime(df['time_interval'])\n",
    "skipping_detector = TollSkippingDetection(df)\n",
    "skipping_report = skipping_detector.detect_potential_toll_skipping()\n",
    "\n",
    "@app.route('/api/toll-skipping', methods=['GET'])\n",
    "def get_toll_skipping():\n",
    "    return jsonify(skipping_report)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
