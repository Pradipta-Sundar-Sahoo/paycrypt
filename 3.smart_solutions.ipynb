{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import datetime\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import uuid\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import pickle\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import networkx as nx\n",
    "import folium\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phase 3: Smart Solutions Implementation...\n",
      "\n",
      "1. LOADING DATA AND MODELS\n",
      "--------------------------------------------------\n",
      "Data loaded with 367074 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting Phase 3: Smart Solutions Implementation...\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD DATA AND MODELS\n",
    "# =============================================================================\n",
    "print(\"\\n1. LOADING DATA AND MODELS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('data/preprocessed_tollplaza_data.csv')\n",
    "df['initiated_time'] = pd.to_datetime(df['initiated_time'])\n",
    "df['time_interval'] = pd.to_datetime(df['time_interval'])\n",
    "\n",
    "# Load models\n",
    "traffic_model = joblib.load('models/traffic_prediction_model.pkl')\n",
    "anomaly_model, anomaly_scaler = joblib.load('models/traffic_anomaly_models.pkl')\n",
    "\n",
    "# Try to load other models if they exist\n",
    "try:\n",
    "    vc_model = joblib.load('models/vehicle_class_prediction_model.pkl')\n",
    "    vc_model_loaded = True\n",
    "except:\n",
    "    vc_model_loaded = False\n",
    "    print(\"Vehicle class model not found, skipping...\")\n",
    "\n",
    "try:\n",
    "    skip_model, skip_scaler = joblib.load('models/toll_skipping_models.pkl')\n",
    "    skip_model_loaded = True\n",
    "except:\n",
    "    skip_model_loaded = False\n",
    "    print(\"Toll skipping model not found, skipping...\")\n",
    "\n",
    "print(f\"Data loaded with {df.shape[0]} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. LANE OPTIMIZATION SYSTEM\n",
      "--------------------------------------------------\n",
      "Lane recommendations for Devanahalli Toll Plaza at 8:00:\n",
      "{\n",
      "  \"plaza\": \"Devanahalli Toll Plaza\",\n",
      "  \"hour\": 8,\n",
      "  \"expected_traffic\": 3656,\n",
      "  \"lanes_needed\": 37,\n",
      "  \"commercial_ratio\": 0.24699124726477023,\n",
      "  \"recommended_lanes\": [\n",
      "    {\n",
      "      \"lane\": \"15\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 172.7101171458999\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"16\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 146.7950024260068\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"14\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 164.6748046875\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"5\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 178.58856191004998\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"4\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 201.40320765334835\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"13\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 210.99536178107607\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"17\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 331.69315326633165\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"18\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 151.23050095117313\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"8\",\n",
      "      \"role\": \"Commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 231.17530140110784\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"12\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 190.78646517739816\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 15,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 160.3962641761174\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"7\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 254.56372218476062\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 16,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 130.01327769347498\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 14,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 135.08233009708738\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"2\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 174.781990521327\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"3\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 236.05408805031448\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"9\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 225.44209636517328\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 5,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 225.7092665788318\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 4,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 158.72906178489703\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 17,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 341.1147776183644\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 13,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 175.56069364161849\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 12,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 204.64171897633994\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"6\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 190.84070796460176\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 8,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 337.5933842239186\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 18,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 144.4778578784758\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 7,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 236.2694840834248\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 2,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 254.82395644283122\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"19\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 171.19322459222082\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 9,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 284.42857142857144\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 3,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 337.259649122807\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 6,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 168.2185582822086\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 19,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 183.38957345971565\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"1\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 255.90424242424243\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 1,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 300.085409252669\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"20\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 180.94389438943895\n",
      "    },\n",
      "    {\n",
      "      \"lane\": 20,\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 300.19069767441863\n",
      "    },\n",
      "    {\n",
      "      \"lane\": \"33\",\n",
      "      \"role\": \"Non-commercial\",\n",
      "      \"expected_volume\": 98,\n",
      "      \"processing_time\": 145.3109756097561\n",
      "    }\n",
      "  ],\n",
      "  \"vehicle_distribution\": {\n",
      "    \"VC4\": 0.8432713347921226,\n",
      "    \"VC7\": 0.053610503282275714,\n",
      "    \"VC20\": 0.026805251641137857,\n",
      "    \"VC5\": 0.02598468271334792,\n",
      "    \"VC10\": 0.013402625820568928,\n",
      "    \"VC12\": 0.010393873085339168,\n",
      "    \"VC11\": 0.009026258205689279,\n",
      "    \"VC9\": 0.007111597374179431,\n",
      "    \"VC13\": 0.0057439824945295405,\n",
      "    \"VC8\": 0.002461706783369803,\n",
      "    \"VC14\": 0.002188183807439825\n",
      "  },\n",
      "  \"historical_average_processing_time\": 176.9789387308534\n",
      "}\n",
      "\n",
      "Dynamic pricing recommendations for Devanahalli Toll Plaza at 8:00:\n",
      "{\n",
      "  \"plaza\": \"Devanahalli Toll Plaza\",\n",
      "  \"hour\": 8,\n",
      "  \"congestion_level\": 0.7615080191626744,\n",
      "  \"current_pricing\": {\n",
      "    \"VC10\": 355.0,\n",
      "    \"VC11\": 540.0,\n",
      "    \"VC12\": 540.0,\n",
      "    \"VC13\": 540.0,\n",
      "    \"VC14\": 540.0,\n",
      "    \"VC20\": 115.0,\n",
      "    \"VC4\": 115.0,\n",
      "    \"VC5\": 175.0,\n",
      "    \"VC7\": 355.0,\n",
      "    \"VC8\": 540.0,\n",
      "    \"VC9\": 175.0\n",
      "  },\n",
      "  \"recommended_pricing\": {\n",
      "    \"VC10\": 369.56,\n",
      "    \"VC11\": 562.14,\n",
      "    \"VC12\": 562.14,\n",
      "    \"VC13\": 562.14,\n",
      "    \"VC14\": 562.14,\n",
      "    \"VC20\": 119.72,\n",
      "    \"VC4\": 119.72,\n",
      "    \"VC5\": 182.18,\n",
      "    \"VC7\": 369.56,\n",
      "    \"VC8\": 562.14,\n",
      "    \"VC9\": 182.18\n",
      "  },\n",
      "  \"expected_impact\": {\n",
      "    \"revenue_change\": \"4.10%\",\n",
      "    \"expected_traffic_reduction\": \"1.31%\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. LANE OPTIMIZATION SYSTEM\n",
    "# =============================================================================\n",
    "print(\"\\n2. LANE OPTIMIZATION SYSTEM\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "class LaneOptimizationSystem:\n",
    "    def __init__(self, data):\n",
    "        self.df = data\n",
    "        self.plaza_lanes = self._get_plaza_lanes()\n",
    "        self.vehicle_types = sorted(self.df['vehicle_class_code'].unique())\n",
    "        self.lane_efficiency = self._calculate_lane_efficiency()\n",
    "        \n",
    "    def _get_plaza_lanes(self) -> Dict[str, List[str]]:\n",
    "        plaza_lanes = {}\n",
    "        \n",
    "        for plaza in self.df['merchant_name'].unique():\n",
    "            lanes = self.df[self.df['merchant_name'] == plaza]['lane'].unique()\n",
    "            plaza_lanes[plaza] = sorted(lanes, key=lambda x: (isinstance(x, str), x))\n",
    "        return plaza_lanes\n",
    "        \n",
    "    def _calculate_lane_efficiency(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate efficiency of each lane.\"\"\"\n",
    "        lane_stats = self.df.groupby(['merchant_name', 'lane'])['inn_rr_time_sec'].agg([\n",
    "            'mean', 'count', 'std'\n",
    "        ]).reset_index()\n",
    "        lane_stats.columns = ['plaza', 'lane', 'avg_processing_time', 'volume', 'time_std']\n",
    "        \n",
    "        # Normalize and compute a weighted efficiency score (lower is better)\n",
    "        scaler = MinMaxScaler()\n",
    "        lane_stats[['norm_time', 'norm_volume']] = scaler.fit_transform(\n",
    "            lane_stats[['avg_processing_time', 'volume']])\n",
    "        \n",
    "        lane_stats['efficiency_score'] = 0.7 * lane_stats['norm_time'] - 0.3 * lane_stats['norm_volume']\n",
    "        return lane_stats\n",
    "    \n",
    "    def get_lane_recommendations(self, plaza: str, hour: int, expected_traffic: int = None) -> Dict[str, Any]:\n",
    "\n",
    "        # Filter for specific plaza\n",
    "        plaza_data = self.df[self.df['merchant_name'] == plaza].copy()\n",
    "        if plaza_data.empty:\n",
    "            return {\"error\": f\"No data available for plaza {plaza}\"}\n",
    "        \n",
    "        # Get available lanes\n",
    "        available_lanes = self.plaza_lanes.get(plaza, [])\n",
    "        if not available_lanes:\n",
    "            return {\"error\": f\"No lanes data available for plaza {plaza}\"}\n",
    "        \n",
    "        # Get historical traffic patterns for this hour\n",
    "        hourly_data = plaza_data[plaza_data['initiated_time'].dt.hour == hour]\n",
    "        if hourly_data.empty:\n",
    "            return {\"error\": f\"No data available for plaza {plaza} at hour {hour}\"}\n",
    "        \n",
    "        # Convert 'vehicle_comvehicle' to numeric if present\n",
    "        if 'vehicle_comvehicle' in hourly_data.columns:\n",
    "            # Replace 'F' with 0 and 'T' with 1; non-convertible values become NaN\n",
    "            hourly_data['vehicle_comvehicle'] = pd.to_numeric(\n",
    "                hourly_data['vehicle_comvehicle'].replace({'F': 0, 'T': 1}),\n",
    "                errors='coerce'\n",
    "            )\n",
    "            commercial_ratio = hourly_data['vehicle_comvehicle'].mean()\n",
    "            if pd.isna(commercial_ratio):\n",
    "                commercial_ratio = 0.3\n",
    "        else:\n",
    "            commercial_ratio = 0.3\n",
    "        \n",
    "        # Get vehicle distribution for this hour\n",
    "        vehicle_dist = hourly_data['vehicle_class_code'].value_counts(normalize=True)\n",
    "        \n",
    "        # Get lane efficiency for this plaza\n",
    "        plaza_efficiency = self.lane_efficiency[self.lane_efficiency['plaza'] == plaza].copy()\n",
    "        \n",
    "        # Calculate needed lanes based on historical or expected traffic\n",
    "        if expected_traffic is None:\n",
    "            expected_traffic = len(hourly_data)\n",
    "        \n",
    "        lanes_needed = max(2, int(np.ceil(expected_traffic / 100)))\n",
    "        lanes_needed = min(lanes_needed, len(available_lanes))  # Can't open more lanes than available\n",
    "        \n",
    "        # Select the most efficient lanes\n",
    "        best_lanes = plaza_efficiency.sort_values('efficiency_score').head(lanes_needed)\n",
    "        \n",
    "        # Determine which lanes to allocate for commercial vehicles\n",
    "        commercial_lanes = max(1, int(np.round(commercial_ratio * lanes_needed)))\n",
    "        \n",
    "        # Sort lanes by efficiency and assign roles\n",
    "        recommended_lanes = []\n",
    "        for i, (_, lane_data) in enumerate(best_lanes.iterrows()):\n",
    "            lane_role = \"Commercial\" if i < commercial_lanes else \"Non-commercial\"\n",
    "            recommended_lanes.append({\n",
    "                \"lane\": lane_data['lane'],\n",
    "                \"role\": lane_role,\n",
    "                \"expected_volume\": int(expected_traffic / lanes_needed),\n",
    "                \"processing_time\": float(lane_data['avg_processing_time'])\n",
    "            })\n",
    "        \n",
    "        # Prepare recommendations\n",
    "        recommendations = {\n",
    "            \"plaza\": plaza,\n",
    "            \"hour\": hour,\n",
    "            \"expected_traffic\": expected_traffic,\n",
    "            \"lanes_needed\": lanes_needed,\n",
    "            \"commercial_ratio\": float(commercial_ratio),\n",
    "            \"recommended_lanes\": recommended_lanes,\n",
    "            \"vehicle_distribution\": {k: float(v) for k, v in vehicle_dist.items()},\n",
    "            \"historical_average_processing_time\": float(hourly_data['inn_rr_time_sec'].mean())\n",
    "        }\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    \n",
    "    def get_dynamic_pricing_recommendations(self, plaza: str, hour: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get dynamic pricing recommendations for a specific plaza and time.\n",
    "        \"\"\"\n",
    "        plaza_data = self.df[self.df['merchant_name'] == plaza].copy()\n",
    "        if plaza_data.empty:\n",
    "            return {\"error\": f\"No data available for plaza {plaza}\"}\n",
    "            \n",
    "        hourly_data = plaza_data[plaza_data['initiated_time'].dt.hour == hour]\n",
    "        if hourly_data.empty:\n",
    "            return {\"error\": f\"No data available for plaza {plaza} at hour {hour}\"}\n",
    "        \n",
    "        max_hourly_traffic = plaza_data.groupby(plaza_data['initiated_time'].dt.hour).size().max()\n",
    "        current_hourly_traffic = len(hourly_data)\n",
    "        congestion_level = current_hourly_traffic / max_hourly_traffic if max_hourly_traffic > 0 else 0\n",
    "        \n",
    "        current_pricing = {}\n",
    "        for vc in self.vehicle_types:\n",
    "            vc_data = hourly_data[hourly_data['vehicle_class_code'] == vc]\n",
    "            if not vc_data.empty:\n",
    "                current_pricing[vc] = float(vc_data['txn_amount'].median())\n",
    "        \n",
    "        recommended_pricing = {}\n",
    "        for vc, base_price in current_pricing.items():\n",
    "            if congestion_level > 0.7:  # High congestion: increase up to 20%\n",
    "                factor = 1.0 + (congestion_level - 0.7) * (0.2 / 0.3)\n",
    "            elif congestion_level < 0.3:  # Low congestion: decrease up to 10%\n",
    "                factor = 1.0 - (0.3 - congestion_level) * (0.1 / 0.3)\n",
    "            else:\n",
    "                factor = 1.0\n",
    "            recommended_pricing[vc] = round(base_price * factor, 2)\n",
    "        \n",
    "        recommendations = {\n",
    "            \"plaza\": plaza,\n",
    "            \"hour\": hour,\n",
    "            \"congestion_level\": float(congestion_level),\n",
    "            \"current_pricing\": current_pricing,\n",
    "            \"recommended_pricing\": recommended_pricing,\n",
    "            \"expected_impact\": {\n",
    "                \"revenue_change\": f\"{(sum(recommended_pricing.values()) - sum(current_pricing.values())) / sum(current_pricing.values()) * 100:.2f}%\",\n",
    "                \"expected_traffic_reduction\": f\"{5 * (congestion_level - 0.5) if congestion_level > 0.5 else 0:.2f}%\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def visualize_lane_recommendations(self, plaza: str, hour: int) -> Dict[str, Any]:\n",
    "    \n",
    "        recommendations = self.get_lane_recommendations(plaza, hour)\n",
    "        if \"error\" in recommendations:\n",
    "            return {\"error\": recommendations[\"error\"]}\n",
    "        \n",
    "        viz_data = {\n",
    "            \"plaza\": plaza,\n",
    "            \"hour\": hour,\n",
    "            \"lane_allocation\": {\n",
    "                \"lanes\": [lane[\"lane\"] for lane in recommendations[\"recommended_lanes\"]],\n",
    "                \"roles\": [lane[\"role\"] for lane in recommendations[\"recommended_lanes\"]],\n",
    "                \"volumes\": [lane[\"expected_volume\"] for lane in recommendations[\"recommended_lanes\"]]\n",
    "            },\n",
    "            \"vehicle_distribution\": recommendations[\"vehicle_distribution\"]\n",
    "        }\n",
    "        \n",
    "        return viz_data\n",
    "\n",
    "# Initialize lane optimization system\n",
    "lane_optimizer = LaneOptimizationSystem(df)\n",
    "\n",
    "# Example usage for lane optimization\n",
    "plaza_example = df['merchant_name'].value_counts().index[0]  # Most common plaza\n",
    "hour_example = 8  # Example hour\n",
    "\n",
    "lane_recommendations = lane_optimizer.get_lane_recommendations(plaza_example, hour_example)\n",
    "print(f\"Lane recommendations for {plaza_example} at {hour_example}:00:\")\n",
    "print(json.dumps(lane_recommendations, indent=2))\n",
    "\n",
    "pricing_recommendations = lane_optimizer.get_dynamic_pricing_recommendations(plaza_example, hour_example)\n",
    "print(f\"\\nDynamic pricing recommendations for {plaza_example} at {hour_example}:00:\")\n",
    "print(json.dumps(pricing_recommendations, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. AUTOMATED INSIGHTS GENERATOR\n",
      "--------------------------------------------------\n",
      "Generating automated insights...\n",
      "\n",
      "Insights Narrative (excerpt):\n",
      "### Bullet-Point Summary\n",
      "\n",
      "- **Traffic Patterns:**\n",
      "  - Peak traffic occurs at 5 PM, with a significant drop-off during the early morning hours.\n",
      "  - Devanahalli Toll Plaza is the busiest, while Magadi Road (P6) Plaza is the quietest.\n",
      "  - The main travel direction is southbound, accounting for nearly half of the traffic.\n",
      "\n",
      "- **Vehicle Insights:**\n",
      "  - Vehicle Class 4 (VC4) dominates the traffic and revenue, comprising over 68% of the total vehicles.\n",
      "  - Commercial vehicles make up 34% of the traffic.\n",
      "  - VC15 has the highest fare, though it represents a tiny fraction of the traffic.\n",
      "\n",
      "- **Operational Efficiency:**\n",
      "  - The average processing time is slightly over 1000 seconds, with significant variance between plazas.\n",
      "  - Bangalore-Nelamangala Plaza is the fastest in processing vehicles, while Magadi Road (P6) Plaza is the slowest.\n",
      "\n",
      "- **Revenue Insights:**\n",
      "  - Devanahalli Toll Plaza generates the highest revenue, contributing over 34% of the total.\n",
      "  - Peak revenue is recorded at 11 PM, suggesting high-value transactions during this hour.\n",
      "\n",
      "### Detailed Analysis\n",
      "\n",
      "**Traffic Patterns:**\n",
      "The data reveals that the busiest time for toll plazas is during the evening rush hour, specifically at 5 PM, with over 21,000 vehicles passing through. This suggests a high volume of commuters returning home. In contrast, the off-peak hour at 3 AM sees significantly less traffic, with just over 6,000 vehicles, indicating reduced demand during late-night hours. The traffic pattern also shows a higher volume in the morning compared to the evening, with a morning-to-evening ratio of 1.27, suggesting morning commutes are slightly more pronounced.\n",
      "\n",
      "The Devanahalli Toll Plaza stands out as the busiest, handling over 84,000 transactions, while the Magadi Road (P6) Plaza is the quietest, with only 573 transactions. This disparity suggests an uneven distribution of traffic, possibly due to the location or connectivity of these plazas. The southbound direction is the main travel route, capturing nearly 50% of the traffic, indicating a potential focus area for infrastructure improvements or traffic management strategies.\n",
      "\n",
      "**Vehicle Insights:**\n",
      "Vehicle Class 4 (VC4) is the predominant vehicle type, accounting for nearly 69% of all vehicles and generating substantial revenue. This class's dominance suggests that toll plazas should optimize their operations to cater to these vehicles, ensuring efficient processing. Commercial vehicles constitute over a third of the traffic, highlighting the importance of maintaining smooth operations to support economic activities.\n",
      "\n",
      "While VC15 has the highest fare, it represents a negligible portion of the traffic, indicating that high-value transactions are rare. This insight can guide pricing strategies and promotional efforts to increase the usage of higher fare classes.\n",
      "\n",
      "**Operational Efficiency:**\n",
      "The average processing time stands at approximately 1005 seconds, but there is a stark contrast between the fastest and slowest plazas. The Bangalore-Nelamangala Plaza processes vehicles quickly, with an average time of 205 seconds, while Magadi Road (P6) Plaza lags significantly, taking over 48,000 seconds. This inefficiency at Magadi Road (P6) Plaza warrants immediate attention, potentially involving process optimization or infrastructure upgrades.\n",
      "\n",
      "**Revenue Insights:**\n",
      "Revenue data highlights that Devanahalli Toll Plaza is the top earner, generating over 34% of the total revenue. This plaza's strategic importance necessitates continued investment and maintenance to sustain its performance. Interestingly, the peak revenue hour is at 11 PM, which may indicate high-value transactions or increased commercial vehicle traffic during this time. Understanding the factors contributing to this peak can help in formulating targeted strategies to boost revenue further.\n",
      "\n",
      "### Actionable Recommendations\n",
      "\n",
      "- **Traffic Management:**\n",
      "  - Implement dynamic pricing strategies to manage peak-hour congestion effectively.\n",
      "  - Enhance traffic flow in the southbound direction through infrastructure improvements or dedicated lanes.\n",
      "\n",
      "- **Operational Improvements:**\n",
      "  - Conduct a thorough review of operations at Magadi Road (P6) Plaza to identify bottlenecks and implement efficiency-enhancing measures.\n",
      "  - Consider technology upgrades or staff training at slower plazas to reduce processing times.\n",
      "\n",
      "- **Revenue Optimization:**\n",
      "  - Explore promotional strategies or discounts to increase the usage of higher fare vehicle classes, such as VC15.\n",
      "  - Analyze the factors leading to high revenue during late-night hours and replicate successful strategies across other time slots.\n",
      "\n",
      "- **Vehicle Accommodation:**\n",
      "  - Optimize facilities and services for VC4 vehicles, given their dominance in traffic and revenue.\n",
      "  - Enhance services for commercial vehicles to support economic activities and ensure smooth operations.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 3. AUTOMATED INSIGHTS GENERATOR\n",
    "# =============================================================================\n",
    "print(\"\\n3. AUTOMATED INSIGHTS GENERATOR\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "class AutomatedInsightsGenerator:\n",
    "    def __init__(self, data, openai_api_key=None):\n",
    "        self.df = data\n",
    "        self.client = None\n",
    "        if openai_api_key:\n",
    "            try:\n",
    "                self.client = OpenAI(api_key=openai_api_key)\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing OpenAI client: {e}\")\n",
    "                print(\"Automated insights will be generated without OpenAI.\")\n",
    "    \n",
    "    def _get_basic_stats(self) -> Dict[str, Any]:\n",
    "        stats = {\n",
    "            \"total_transactions\": len(self.df),\n",
    "            \"total_plazas\": self.df['merchant_name'].nunique(),\n",
    "            \"total_vehicles\": self.df['vehicle_regn_number'].nunique(),\n",
    "            \"total_revenue\": float(self.df['txn_amount'].sum()),\n",
    "            \"avg_transaction_amount\": float(self.df['txn_amount'].mean()),\n",
    "            \"busiest_plaza\": self.df['merchant_name'].value_counts().index[0],\n",
    "            \"busiest_hour\": self.df.groupby(self.df['initiated_time'].dt.hour).size().idxmax(),\n",
    "            \"most_common_vehicle_type\": self.df['vehicle_class_code'].value_counts().index[0]\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def _get_traffic_insights(self) -> Dict[str, Any]:\n",
    "        hourly_traffic = self.df.groupby(self.df['initiated_time'].dt.hour).size()\n",
    "        peak_hour = hourly_traffic.idxmax()\n",
    "        off_peak_hour = hourly_traffic.idxmin()\n",
    "        morning_traffic = hourly_traffic.loc[6:12].sum()\n",
    "        evening_traffic = hourly_traffic.loc[16:20].sum()\n",
    "        night_traffic = hourly_traffic.loc[[*range(0, 6), *range(21, 24)]].sum()\n",
    "        plaza_traffic = self.df['merchant_name'].value_counts()\n",
    "        busiest_plaza = plaza_traffic.index[0]\n",
    "        quietest_plaza = plaza_traffic.index[-1]\n",
    "        direction_traffic = self.df['direction'].value_counts(normalize=True)\n",
    "        main_direction = direction_traffic.index[0]\n",
    "        main_direction_pct = float(direction_traffic.iloc[0] * 100)\n",
    "        \n",
    "        insights = {\n",
    "            \"peak_hour\": int(peak_hour),\n",
    "            \"peak_hour_traffic\": int(hourly_traffic[peak_hour]),\n",
    "            \"off_peak_hour\": int(off_peak_hour),\n",
    "            \"off_peak_hour_traffic\": int(hourly_traffic[off_peak_hour]),\n",
    "            \"peak_to_offpeak_ratio\": float(hourly_traffic[peak_hour] / hourly_traffic[off_peak_hour]),\n",
    "            \"morning_vs_evening\": {\n",
    "                \"morning_traffic\": int(morning_traffic),\n",
    "                \"evening_traffic\": int(evening_traffic),\n",
    "                \"ratio\": float(morning_traffic / evening_traffic) if evening_traffic > 0 else float('inf')\n",
    "            },\n",
    "            \"night_traffic_percentage\": float(night_traffic / hourly_traffic.sum() * 100),\n",
    "            \"busiest_plaza\": busiest_plaza,\n",
    "            \"busiest_plaza_transactions\": int(plaza_traffic[busiest_plaza]),\n",
    "            \"quietest_plaza\": quietest_plaza,\n",
    "            \"quietest_plaza_transactions\": int(plaza_traffic[quietest_plaza]),\n",
    "            \"main_travel_direction\": main_direction,\n",
    "            \"main_direction_percentage\": main_direction_pct\n",
    "        }\n",
    "        return insights\n",
    "    \n",
    "    def _get_vehicle_insights(self) -> Dict[str, Any]:\n",
    "        vehicle_dist = self.df['vehicle_class_code'].value_counts(normalize=True)\n",
    "        \n",
    "        if 'vehicle_comvehicle' in self.df.columns:\n",
    "            # Convert the column to numeric: 'F' -> 0, 'T' -> 1, non-convertible values become NaN\n",
    "            vc_series = pd.to_numeric(\n",
    "                self.df['vehicle_comvehicle'].replace({'F': 0, 'T': 1}),\n",
    "                errors='coerce'\n",
    "            )\n",
    "            commercial_pct = float(vc_series.mean() * 100)\n",
    "        else:\n",
    "            commercial_classes = ['VC5', 'VC6', 'VC7', 'VC8', 'VC10', 'VC11', 'VC12', 'VC13', 'VC14', 'VC15', 'VC16', 'VC17', 'VC20']\n",
    "            commercial_pct = float(self.df['vehicle_class_code'].isin(commercial_classes).mean() * 100)\n",
    "        \n",
    "        revenue_by_class = self.df.groupby('vehicle_class_code')['txn_amount'].sum()\n",
    "        top_revenue_class = revenue_by_class.idxmax()\n",
    "        avg_by_class = self.df.groupby('vehicle_class_code')['txn_amount'].mean().sort_values(ascending=False)\n",
    "        \n",
    "        insights = {\n",
    "            \"top_vehicle_class\": vehicle_dist.index[0],\n",
    "            \"top_vehicle_class_percentage\": float(vehicle_dist.iloc[0] * 100),\n",
    "            \"commercial_vehicle_percentage\": commercial_pct,\n",
    "            \"top_revenue_vehicle_class\": top_revenue_class,\n",
    "            \"top_revenue_vehicle_class_amount\": float(revenue_by_class[top_revenue_class]),\n",
    "            \"highest_fare_vehicle_class\": avg_by_class.index[0],\n",
    "            \"highest_fare_amount\": float(avg_by_class.iloc[0]),\n",
    "            \"vehicle_class_distribution\": {k: float(v * 100) for k, v in vehicle_dist.items()}\n",
    "        }\n",
    "        return insights\n",
    "\n",
    "    \n",
    "    def _get_operational_insights(self) -> Dict[str, Any]:\n",
    "        avg_processing = float(self.df['inn_rr_time_sec'].mean())\n",
    "        plaza_processing = self.df.groupby('merchant_name')['inn_rr_time_sec'].mean().sort_values()\n",
    "        fastest_plaza = plaza_processing.index[0]\n",
    "        slowest_plaza = plaza_processing.index[-1]\n",
    "        class_processing = self.df.groupby('vehicle_class_code')['inn_rr_time_sec'].mean().sort_values()\n",
    "        fastest_class = class_processing.index[0]\n",
    "        slowest_class = class_processing.index[-1]\n",
    "        hour_processing = self.df.groupby(self.df['initiated_time'].dt.hour)['inn_rr_time_sec'].mean()\n",
    "        fastest_hour = hour_processing.idxmin()\n",
    "        slowest_hour = hour_processing.idxmax()\n",
    "        \n",
    "        insights = {\n",
    "            \"average_processing_time\": avg_processing,\n",
    "            \"fastest_plaza\": fastest_plaza,\n",
    "            \"fastest_plaza_time\": float(plaza_processing[fastest_plaza]),\n",
    "            \"slowest_plaza\": slowest_plaza,\n",
    "            \"slowest_plaza_time\": float(plaza_processing[slowest_plaza]),\n",
    "            \"fastest_vehicle_class\": fastest_class,\n",
    "            \"fastest_vehicle_class_time\": float(class_processing[fastest_class]),\n",
    "            \"slowest_vehicle_class\": slowest_class,\n",
    "            \"slowest_vehicle_class_time\": float(class_processing[slowest_class]),\n",
    "            \"fastest_hour\": int(fastest_hour),\n",
    "            \"fastest_hour_time\": float(hour_processing[fastest_hour]),\n",
    "            \"slowest_hour\": int(slowest_hour),\n",
    "            \"slowest_hour_time\": float(hour_processing[slowest_hour])\n",
    "        }\n",
    "        return insights\n",
    "    \n",
    "    def _get_revenue_insights(self) -> Dict[str, Any]:\n",
    "        hourly_revenue = self.df.groupby(self.df['initiated_time'].dt.hour)['txn_amount'].sum()\n",
    "        peak_revenue_hour = hourly_revenue.idxmax()\n",
    "        plaza_revenue = self.df.groupby('merchant_name')['txn_amount'].sum().sort_values(ascending=False)\n",
    "        top_revenue_plaza = plaza_revenue.index[0]\n",
    "        plaza_avg_revenue = self.df.groupby('merchant_name')['txn_amount'].mean().sort_values(ascending=False)\n",
    "        highest_avg_plaza = plaza_avg_revenue.index[0]\n",
    "        \n",
    "        insights = {\n",
    "            \"total_daily_revenue\": float(self.df['txn_amount'].sum()),\n",
    "            \"average_transaction_amount\": float(self.df['txn_amount'].mean()),\n",
    "            \"median_transaction_amount\": float(self.df['txn_amount'].median()),\n",
    "            \"peak_revenue_hour\": int(peak_revenue_hour),\n",
    "            \"peak_hour_revenue\": float(hourly_revenue[peak_revenue_hour]),\n",
    "            \"top_revenue_plaza\": top_revenue_plaza,\n",
    "            \"top_plaza_revenue\": float(plaza_revenue[top_revenue_plaza]),\n",
    "            \"top_plaza_revenue_percentage\": float(plaza_revenue[top_revenue_plaza] / plaza_revenue.sum() * 100),\n",
    "            \"highest_average_revenue_plaza\": highest_avg_plaza,\n",
    "            \"highest_average_amount\": float(plaza_avg_revenue[highest_avg_plaza])\n",
    "        }\n",
    "        return insights\n",
    "    \n",
    "    def generate_natural_language_insights(self, data_summary: Dict[str, Any]) -> str:\n",
    "        if self.client:\n",
    "            try:\n",
    "                prompt = f\"\"\"\n",
    "                You are a transportation analytics expert. Based on the following toll plaza data insights, \n",
    "                generate a comprehensive analysis that highlights key patterns, anomalies, and actionable \n",
    "                recommendations for toll plaza operators. Format your response as a bullet-point summary\n",
    "                followed by paragraphs of detailed analysis. Focus on actionable insights.\n",
    "                \n",
    "                DATA INSIGHTS:\n",
    "                {data_summary}\n",
    "                \"\"\"\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[{\"role\": \"system\", \"content\": \"You are a toll plaza analytics expert.\"},\n",
    "                              {\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.5\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error using OpenAI: {e}\")\n",
    "                return self._generate_fallback_insights(data_summary)\n",
    "        else:\n",
    "            return self._generate_fallback_insights(data_summary)\n",
    "    \n",
    "    def _generate_fallback_insights(self, data_summary: Dict[str, Any]) -> str:\n",
    "        traffic = data_summary.get(\"traffic_insights\", {})\n",
    "        vehicle = data_summary.get(\"vehicle_insights\", {})\n",
    "        operation = data_summary.get(\"operational_insights\", {})\n",
    "        revenue = data_summary.get(\"revenue_insights\", {})\n",
    "        \n",
    "        insights = f\"\"\"\n",
    "        # Toll Plaza Analytics Insights\n",
    "\n",
    "        ## Key Highlights\n",
    "        * Total transactions: {data_summary.get('basic_stats', {}).get('total_transactions', 'N/A')}\n",
    "        * Total revenue: ₹{data_summary.get('basic_stats', {}).get('total_revenue', 'N/A'):,.2f}\n",
    "        * Peak hour: {traffic.get('peak_hour', 'N/A')}:00 with {traffic.get('peak_hour_traffic', 'N/A')} transactions\n",
    "        * Busiest plaza: {traffic.get('busiest_plaza', 'N/A')} with {traffic.get('busiest_plaza_transactions', 'N/A')} transactions\n",
    "        \n",
    "        ## Traffic Patterns\n",
    "        The peak hour ({traffic.get('peak_hour', 'N/A')}:00) has {traffic.get('peak_to_offpeak_ratio', 'N/A'):.1f}x more traffic than the off-peak hour ({traffic.get('off_peak_hour', 'N/A')}:00). \n",
    "        Morning traffic is {traffic.get('morning_vs_evening', {}).get('ratio', 'N/A'):.2f}x the evening traffic.\n",
    "        \n",
    "        ## Vehicle Distribution\n",
    "        {vehicle.get('commercial_vehicle_percentage', 'N/A'):.1f}% of vehicles are commercial, with {vehicle.get('top_vehicle_class', 'N/A')} being the most common type.\n",
    "        The {vehicle.get('top_revenue_vehicle_class', 'N/A')} class generates the most revenue at ₹{vehicle.get('top_revenue_vehicle_class_amount', 'N/A'):,.2f}.\n",
    "        \n",
    "        ## Operational Efficiency\n",
    "        Average processing time is {operation.get('average_processing_time', 'N/A'):.2f} seconds.\n",
    "        {operation.get('fastest_plaza', 'N/A')} is the most efficient plaza at {operation.get('fastest_plaza_time', 'N/A'):.2f}s.\n",
    "        \n",
    "        ## Revenue Insights\n",
    "        Total daily revenue is ₹{revenue.get('total_daily_revenue', 'N/A'):,.2f} with an average transaction of ₹{revenue.get('average_transaction_amount', 'N/A'):.2f}.\n",
    "        Peak revenue hour is {revenue.get('peak_revenue_hour', 'N/A')}:00, generating ₹{revenue.get('peak_hour_revenue', 'N/A'):,.2f}.\n",
    "        \n",
    "        ## Recommendations\n",
    "        1. Optimize lane allocation during peak hours.\n",
    "        2. Consider dynamic pricing to balance traffic.\n",
    "        3. Improve processing times at less efficient plazas.\n",
    "        4. Target revenue optimization for {vehicle.get('top_revenue_vehicle_class', 'N/A')} vehicles.\n",
    "        \"\"\"\n",
    "        return insights\n",
    "    \n",
    "    def generate_insights_report(self) -> Dict[str, Any]:\n",
    "        basic_stats = self._get_basic_stats()\n",
    "        traffic_insights = self._get_traffic_insights()\n",
    "        vehicle_insights = self._get_vehicle_insights()\n",
    "        operational_insights = self._get_operational_insights()\n",
    "        revenue_insights = self._get_revenue_insights()\n",
    "        \n",
    "        data_summary = {\n",
    "            \"basic_stats\": basic_stats,\n",
    "            \"traffic_insights\": traffic_insights,\n",
    "            \"vehicle_insights\": vehicle_insights,\n",
    "            \"operational_insights\": operational_insights,\n",
    "            \"revenue_insights\": revenue_insights\n",
    "        }\n",
    "        \n",
    "        narrative = self.generate_natural_language_insights(data_summary)\n",
    "        \n",
    "        report = {\n",
    "            \"summary\": data_summary,\n",
    "            \"narrative\": narrative,\n",
    "            \"generated_at\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"data_date\": self.df['initiated_time'].dt.date.min().strftime(\"%Y-%m-%d\")\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def generate_plaza_insights(self, plaza_name: str) -> Dict[str, Any]:\n",
    "        plaza_data = self.df[self.df['merchant_name'] == plaza_name]\n",
    "        if plaza_data.empty:\n",
    "            return {\"error\": f\"No data found for plaza {plaza_name}\"}\n",
    "        \n",
    "        temp_generator = AutomatedInsightsGenerator(plaza_data, None)\n",
    "        return temp_generator.generate_insights_report()\n",
    "\n",
    "# Check for OpenAI key (implement your own key handling)\n",
    "openai_key = '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'\n",
    "if not openai_key:\n",
    "    print(\"No OpenAI API key found. Using fallback insight generation.\")\n",
    "\n",
    "# Initialize insights generator\n",
    "insights_generator = AutomatedInsightsGenerator(df, openai_key)\n",
    "print(\"Generating automated insights...\")\n",
    "insights_report = insights_generator.generate_insights_report()\n",
    "print(\"\\nInsights Narrative (excerpt):\")\n",
    "narrative_excerpt = \"\\n\".join(insights_report[\"narrative\"].strip().split(\"\\n\"))\n",
    "print(narrative_excerpt + \"\\n...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'preprocessed_tollplaza_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m incidents\n\u001b[32m     62\u001b[39m app = Flask(\u001b[34m__name__\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpreprocessed_tollplaza_data.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33minitiated_time\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33minitiated_time\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     65\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtime_interval\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mtime_interval\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pradi\\OneDrive\\Desktop\\Paycrypt !\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pradi\\OneDrive\\Desktop\\Paycrypt !\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pradi\\OneDrive\\Desktop\\Paycrypt !\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pradi\\OneDrive\\Desktop\\Paycrypt !\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pradi\\OneDrive\\Desktop\\Paycrypt !\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'preprocessed_tollplaza_data.csv'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from flask import Flask, jsonify\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def debug_dataframe(df):\n",
    "    logger.info(\"Debugging DataFrame...\")\n",
    "    logger.info(f\"Columns: {df.columns.tolist()}\")\n",
    "    logger.info(f\"Data types: {df.dtypes}\")\n",
    "    logger.info(f\"Missing values: {df.isna().sum()}\")\n",
    "    logger.info(f\"First few rows:\\n{df.head()}\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "class TollSkippingDetection:\n",
    "    def __init__(self, data):\n",
    "        self.df = debug_dataframe(data)  # ✅ Ensure self.df is initialized\n",
    "        self.graph = self._build_route_graph()\n",
    "        self.expected_routes = self._identify_common_routes()\n",
    "        self.tag_routes = self._extract_vehicle_routes()\n",
    "\n",
    "    def _build_route_graph(self):\n",
    "        G = nx.DiGraph()\n",
    "        for _, group in self.df.groupby('tag_id'):\n",
    "            plazas = group.sort_values('initiated_time')['merchant_name'].tolist()\n",
    "            for i in range(len(plazas) - 1):\n",
    "                G.add_edge(plazas[i], plazas[i + 1], weight=G.get_edge_data(plazas[i], plazas[i + 1], {'weight': 0})['weight'] + 1)\n",
    "        return G\n",
    "\n",
    "    def _identify_common_routes(self):\n",
    "        common_routes = {}\n",
    "        for edge in self.graph.edges(data=True):\n",
    "            source, target, data = edge\n",
    "            common_routes[(source, target)] = list(nx.all_simple_paths(self.graph, source, target))  # ✅ Convert generator to list\n",
    "        return common_routes\n",
    "\n",
    "    def _extract_vehicle_routes(self):\n",
    "        vehicle_routes = {}\n",
    "        for tag_id, group in self.df.groupby('tag_id'):\n",
    "            sorted_group = group.sort_values('initiated_time')\n",
    "            vehicle_routes[tag_id] = list(zip(sorted_group['merchant_name'], sorted_group['initiated_time']))\n",
    "        return vehicle_routes\n",
    "\n",
    "    def detect_potential_toll_skipping(self):\n",
    "        incidents = []\n",
    "        for tag_id, route in self.tag_routes.items():\n",
    "            for i in range(len(route) - 1):\n",
    "                source, target = route[i][0], route[i + 1][0]\n",
    "                if (source, target) in self.expected_routes and len(self.expected_routes[(source, target)]) > 2:\n",
    "                    incidents.append({\n",
    "                        \"tag_id\": tag_id,\n",
    "                        \"source_plaza\": source,\n",
    "                        \"target_plaza\": target,\n",
    "                        \"skipped_tolls\": len(self.expected_routes[(source, target)]) - 2,\n",
    "                        \"estimated_loss\": (len(self.expected_routes[(source, target)]) - 2) * 50\n",
    "                    })\n",
    "        return incidents\n",
    "\n",
    "app = Flask(__name__)\n",
    "df = pd.read_csv('preprocessed_tollplaza_data.csv')\n",
    "df['initiated_time'] = pd.to_datetime(df['initiated_time'])\n",
    "df['time_interval'] = pd.to_datetime(df['time_interval'])\n",
    "skipping_detector = TollSkippingDetection(df)\n",
    "skipping_report = skipping_detector.detect_potential_toll_skipping()\n",
    "\n",
    "@app.route('/api/toll-skipping', methods=['GET'])\n",
    "def get_toll_skipping():\n",
    "    return jsonify(skipping_report)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
